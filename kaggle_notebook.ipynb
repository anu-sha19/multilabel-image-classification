{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":74788,"databundleVersionId":8176112,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Goal of the project \n\n# input an image from the test set\n# The model will output predicted labels (e.g., [1, 8, 19]).\n# These labels will then be mapped to a caption using:\n #- A pre-defined lookup table for captions associated with labels.\n #- or a text generation model \n\ntrain = '/kaggle/input/multi-label-classification-competition-2024/COMP5329S1A2Dataset/train.csv'\ntest = '/kaggle/input/multi-label-classification-competition-2024/COMP5329S1A2Dataset/test.csv'\nimage_dir = '/kaggle/input/multi-label-classification-competition-2024/COMP5329S1A2Dataset/data'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nfrom io import StringIO","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(train) as file:\n    lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\ntrain_df = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(test) as file:\n    lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\ntest_df = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_df['Labels'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_labels = train_df['Labels'].apply(lambda x: x.split()).explode().unique()\n\n# Checking how many unique labels exist (should be between 1 and 19, excluding 12)\nlen(all_labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nall_possible_labels = set(map(str, range(1, 20)))\nmissing_labels = all_possible_labels - set(all_labels)\n\n# Output the missing label\nmissing_labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer\nimport numpy as np\n # converting all the labels into a binary matrix\n\ntrain_df['Labels'] = train_df['Labels'].apply(lambda x: list(map(int, x.split())))\n\n# all classes (e.g., excluding class 12)\nall_classes = [i for i in range(1, 20) if i != 12]\n\n# binary matrix\nmlb = MultiLabelBinarizer(classes=all_classes)\nbinary_labels = mlb.fit_transform(train_df['Labels'])\n\nnp.save('train_labels.npy', binary_labels)\nprint(f\"Binary labels saved: {binary_labels.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loaded_labels = np.load('train_labels.npy')\nprint(loaded_labels)\nprint(loaded_labels.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_index = 0  # checking for image 1 \nprint(\"Original Labels:\", train_df['Labels'][sample_index])\nprint(\"Binary Representation:\", binary_labels[sample_index])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom torchvision import transforms\nfrom PIL import Image\n# processing all images at once caused the kernel to restart again and again\n# so doing it in chunks of 5000 images at a time.\n\n# preprocessing pipeline\nimage_transform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize\n    transforms.ToTensor(),          # Convert to tensor\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n])\n\ndef preprocess_images_in_chunk(image_dir, filenames, chunk_size, output_dir):\n    \"\"\"\n    Preprocess images in smaller chunks and save each chunk separately.\n    Args:\n        image_dir: Path to the folder containing images.\n        filenames: List of image filenames to process.\n        chunk_size: Number of images to process in one chunk.\n        output_dir: Directory to save the preprocessed chunks.\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    total_files = len(filenames)\n    \n    for start_idx in range(0, total_files, chunk_size):\n        end_idx = min(start_idx + chunk_size, total_files)\n        chunk_filenames = filenames[start_idx:end_idx]\n        \n        image_data = []\n        for img_name in chunk_filenames:\n            img_path = os.path.join(image_dir, img_name)\n            img_tensor = image_transform(Image.open(img_path).convert('RGB'))\n            image_data.append(img_tensor)\n        \n        # Save the current chunk\n        chunk_file = os.path.join(output_dir, f'preprocessed_images_{start_idx}_{end_idx}.pt')\n        torch.save(torch.stack(image_data), chunk_file)\n        print(f\"Saved chunk {start_idx}-{end_idx} to {chunk_file}\")\n\n\ntrain_filenames = train_df['ImageID'].tolist()\nchunk_size = 5000  # Process 5,000 images at a time\noutput_dir = './preprocessed_train_chunks'\n\npreprocess_images_in_chunk(image_dir, train_filenames, chunk_size, output_dir)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_test_images_in_parts(image_dir, filenames, output_dir, part_size=1000):\n    \"\"\"\n    Preprocess test images and save in smaller parts.\n    Args:\n        image_dir: Path to the folder containing images.\n        filenames: List of test image filenames.\n        output_dir: Directory to save parts.\n        part_size: Number of images per part.\n    \"\"\"\n    import os\n    os.makedirs(output_dir, exist_ok=True)\n\n    for start_idx in range(0, len(filenames), part_size):\n        end_idx = min(start_idx + part_size, len(filenames))\n        part_filenames = filenames[start_idx:end_idx]\n        \n        # Process images in the current part\n        image_data = []\n        for img_name in part_filenames:\n            img_path = os.path.join(image_dir, img_name)\n            img_tensor = image_transform(Image.open(img_path).convert('RGB'))\n            image_data.append(img_tensor)\n        \n        part_tensor = torch.stack(image_data)\n        part_file = os.path.join(output_dir, f'test_images_part_{start_idx}_{end_idx}.pt')\n        torch.save(part_tensor, part_file)\n        print(f\"Saved test images part {start_idx}-{end_idx} to {part_file}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}